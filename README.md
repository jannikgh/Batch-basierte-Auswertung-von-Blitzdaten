# Batch-basierte Auswertung von Blitzdaten

Dieses Projekt implementiert eine **Batch-Datenpipeline** zur Verarbeitung von BlitzaktivitÃ¤tsdaten auf Basis der **Google Cloud Platform (GCP)**.  
Die Architektur besteht aus folgenden Schritten:

1. **Upload**: Ein Python-Skript lÃ¤dt CSV-Dateien mit Blitzdaten in einen Google Cloud Storage (GCS) Bucket.  
2. **Processing**: Ein Dataflow-Template (`CSV to BigQuery`) liest die CSV-Dateien, validiert und verarbeitet sie.  
3. **Storage**: Die bereinigten Daten werden in **BigQuery** gespeichert und stehen dort fÃ¼r Analysen oder Machine Learning (Out of Scope) bereit.  

---

## ğŸ“‚ Repository-Inhalt
- `dataflow_pipeline.py` â†’ Python-Skript fÃ¼r den Upload in GCS  
- `lightning_strikes_schema_bq.json` â†’ BigQuery-Schema fÃ¼r die Tabelle  
- `requirements.txt` â†’ Python-AbhÃ¤ngigkeiten (z. B. `google-cloud-storage`, `apache-beam[gcp]`)  
- `Dockerfile` â†’ Containerisierung des Skripts  
- `LICENSE` â†’ Apache-2.0 Lizenz  

---

## ğŸš€ Nutzung

1. Docker Image bauen
```bash
docker build -t lightning-uploader .
2. Hilfe anzeigen
bash
Code kopieren
docker run --rm lightning-uploader --help
3. CSV-Datei nach GCS hochladen
bash
Code kopieren
docker run --rm \
  -e GOOGLE_APPLICATION_CREDENTIALS=/creds/key.json \
  -v /Users/jannikgross-hardt/Desktop/Batch-basierte-Auswertung-von-Blitzdaten/key.json:/creds/key.json:ro \
  -v /Users/jannikgross-hardt/Desktop/Batch-basierte-Auswertung-von-Blitzdaten/lightning_strikes_dataset.csv:/data/file.csv:ro \
  lightning-uploader \
    --local-file /data/file.csv \
    --bucket blitzdaten_us1 \
    --dest-prefix input/ \
    --rename-with-timestamp

## ğŸ”’ Sicherheit (IAM)
FÃ¼r den Zugriff werden Service Accounts genutzt:

Zum Upload benÃ¶tigt der Service Account mindestens die Rolle Storage Object Admin.

FÃ¼r Lesezugriffe (Apache Beam) zusÃ¤tzlich Storage Object Viewer.

## ğŸ“ Lizenz
Dieses Projekt steht unter der Apache-2.0 Lizenz.

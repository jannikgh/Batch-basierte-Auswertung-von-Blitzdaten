# Batch-basierte Auswertung von Blitzdaten

Dieses Projekt implementiert eine skalierbare, wartbare und zuverlÃ¤ssige Datenarchitektur zur **Batch-basierten Verarbeitung von BlitzaktivitÃ¤tsdaten**.  
Die Architektur basiert auf Python-Skripten, Google Cloud Storage (GCS), einer Apache Beam Pipeline (DirectRunner) und BigQuery.  

Das Projekt wurde im Rahmen des IU-Kurses **Data Engineering (DLMDWWDE02)** umgesetzt.

---

## ğŸš€ ArchitekturÃ¼bersicht

**Datenfluss:**
1. CSV-Datei (`lightning_strikes_dataset.csv`) mit BlitzaktivitÃ¤tsdaten (3,4 Mio. EintrÃ¤ge).  
2. **Uploader-Skript (`uploader.py`)** validiert die Datei und lÃ¤dt sie nach **Google Cloud Storage** (`gs://blitzdaten_us1/input/`).  
3. **Dataflow-Pipeline (`dataflow_pipeline.py`)** liest die CSV aus GCS, parst und bereinigt die Daten.  
4. Ergebnisse werden in **BigQuery** gespeichert (`blitzdaten_us1.lightning_strikes_us1_v2`).  

**Komponenten:**
- Python 3.10  
- Apache Beam (DirectRunner = lokal)  
- Google Cloud Storage (Input-Datei, Temp-/Staging-Bucket)  
- BigQuery (Datenbank fÃ¼r Analyse und ML)  
- IAM & KMS fÃ¼r Sicherheit  

---

## âš™ï¸ Setup

```bash
# Python 3.10 verwenden (nicht 3.13!)
python3.10 -m venv venv
source venv/bin/activate

# AbhÃ¤ngigkeiten installieren
pip install -r requirements.txt

Erstelle die Datei requirements.txt mit folgendem Inhalt (vereinfacht):

apache-beam[gcp]
google-cloud-storage

ğŸ“‚ Nutzung der Skripte

1. Daten hochladen

Validieren & Upload der CSV nach GCS:
python uploader.py

2. Pipeline lokal ausfÃ¼hren (DirectRunner)
python dataflow_pipeline.py


Die Pipeline liest aus GCS, verarbeitet lokal (DirectRunner) und schreibt die Ergebnisse nach BigQuery.

ğŸ“Š Ergebnisse prÃ¼fen

Nach dem erfolgreichen Pipeline-Lauf kÃ¶nnen die Ergebnisse in BigQuery abgefragt werden, z. B.:

-- Anzahl der geladenen Zeilen
SELECT COUNT(*) 
FROM `agile-bonbon-470410-j2.blitzdaten_us1.lightning_strikes_us1_v2`;

-- Summe der Blitze pro Tag
SELECT date, SUM(number_of_strikes) AS strikes
FROM `agile-bonbon-470410-j2.blitzdaten_us1.lightning_strikes_us1_v2`
GROUP BY date
ORDER BY date ASC;

ğŸ” Sicherheit

Zugriff auf GCS und BigQuery erfolgt ausschlieÃŸlich Ã¼ber einen Service Account mit eingeschrÃ¤nkten Rollen:

roles/storage.admin (fÃ¼r GCS)

roles/bigquery.dataEditor (fÃ¼r BigQuery)

Bucket und Dataset sind nicht Ã¶ffentlich.

Daten werden automatisch durch Google Cloud verschlÃ¼sselt (KMS).

ğŸ” Reproduzierbarkeit & Wartbarkeit

Versionskontrolle: gesamter Code im GitHub-Repository.

requirements.txt: ermÃ¶glicht reproduzierbare Python-Umgebung.

Logging: Parsing-Fehler werden im Pipeline-Skript protokolliert.

Optionale Erweiterung: ein Dockerfile kann hinzugefÃ¼gt werden, um die Pipeline in Containern laufen zu lassen.

ğŸ“Œ Hinweise

Das Projekt wurde mit DirectRunner (lokal) umgesetzt, wie in der PrÃ¼fungsaufgabe gefordert.

ZusÃ¤tzlich wurde die Pipeline erfolgreich mit DataflowRunner (Cloud) getestet, wobei EinschrÃ¤nkungen durch IAM-Policies berÃ¼cksichtigt werden mussten.

Damit ist die Architektur sowohl prÃ¼fungsrelevant (lokal) als auch realistisch (Cloud) nutzbar.
